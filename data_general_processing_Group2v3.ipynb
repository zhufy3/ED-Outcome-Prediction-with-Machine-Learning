{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing for healthcare analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from helpers6 import *\n",
    "from past_icu_count import *\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(448972, 120)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Just place the dataset in the same folder as the notebook\n",
    "df = pd.read_csv('master_dataset_2.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out missing age and gender records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(448135, 120)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter age or gender is NaN (837 records)\n",
    "df = df[~df['age'].isna()]\n",
    "df = df[~df['gender'].isna()]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out missing acuity (disabled on requested by YX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter out triage_acuity with null values\n",
    "# df = df[df['triage_acuity'].notnull()]\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrections to past 30/90/365d ICU stays from ED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuyi\\aha\\Project Notebooks\\past_icu_count.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_icu.intime_ed = pd.to_datetime(df_icu.intime_ed) # convert intime to datetime type\n",
      "C:\\Users\\zhuyi\\aha\\Project Notebooks\\past_icu_count.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_icu.intime_icu = pd.to_datetime(df_icu.intime_icu) # convert intime to datetime type\n",
      "C:\\Users\\zhuyi\\aha\\Project Notebooks\\past_icu_count.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_icu.intime_ed = pd.to_datetime(df_icu.intime_ed) # convert intime to datetime type\n",
      "C:\\Users\\zhuyi\\aha\\Project Notebooks\\past_icu_count.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_icu.intime_icu = pd.to_datetime(df_icu.intime_icu) # convert intime to datetime type\n",
      "C:\\Users\\zhuyi\\aha\\Project Notebooks\\past_icu_count.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_icu.intime_ed = pd.to_datetime(df_icu.intime_ed) # convert intime to datetime type\n",
      "C:\\Users\\zhuyi\\aha\\Project Notebooks\\past_icu_count.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_icu.intime_icu = pd.to_datetime(df_icu.intime_icu) # convert intime to datetime type\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(448135, 120)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = count_n_days(df, 30)\n",
    "df = count_n_days(df, 90)\n",
    "df = count_n_days(df, 365)\n",
    "df.shape\n",
    "# no more duplicates found in this version, past ICU stays mapped using index (unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out patients who died before entering ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(448117, 120)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out rows where before_ed_mortality is 1 (18 records)\n",
    "df = df[df['before_ed_mortality'] != 1]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out patients who died during ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(448115, 120)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out rows where ed_death is 1 (2 records)\n",
    "df = df[df['ed_death'] != 1]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop duplicate hospital entries on different ed entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(234033, 120) \n",
      " (214082, 120) \n",
      " (213480, 120) \n",
      " (447513, 120)\n"
     ]
    }
   ],
   "source": [
    "# filter rows where 'hadm_id' is null\n",
    "df_null = df[df['hadm_id'].isnull()]\n",
    "\n",
    "# filter rows where 'hadm_id' is not null\n",
    "df_not_null = df[df['hadm_id'].notnull()]\n",
    "\n",
    "# Use drop_duplicates to keep the latest occurrence of each 'hadm_id'\n",
    "df_result = df_not_null.drop_duplicates(subset=['hadm_id'], keep='last')\n",
    "\n",
    "# Combine the original DataFrame with df_not_null and the duplicates-filtered DataFrame\n",
    "df = pd.concat([df_null, df_result], ignore_index=True)\n",
    "\n",
    "print(df_null.shape, \"\\n\", df_not_null.shape, \"\\n\", df_result.shape, \"\\n\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop errorneous entries for time_to_icu_transfer_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447502, 120)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter rows where 'time_to_icu_transfer_hours' is null\n",
    "df_null = df[df['time_to_icu_transfer_hours'].isnull()]\n",
    "\n",
    "# filter rows where 'time_to_icu_transfer_hours' is not null\n",
    "df_not_null = df[df['time_to_icu_transfer_hours'].notnull()]\n",
    "\n",
    "# Filter the DataFrame where time_to_icu_transfer_hours >= -24\n",
    "df_result = df_not_null[df_not_null['time_to_icu_transfer_hours'] >= -24]\n",
    "\n",
    "# Combine the original DataFrame with df_not_null and the duplicates-filtered DataFrame\n",
    "df = pd.concat([df_null, df_result], ignore_index=True)\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop errorneous entries for difference between 'admittime' and 'intime_ed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447444, 120)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'admittime' and 'intime_ed' to datetime format\n",
    "df['admittime'] = pd.to_datetime(df['admittime'])\n",
    "df['intime_ed'] = pd.to_datetime(df['intime_ed'])\n",
    "\n",
    "# filter rows where 'admittime' or 'admittime' is null\n",
    "df_null = df[df['admittime'].isnull() | df['intime_ed'].isnull()]\n",
    "\n",
    "# filter rows where 'admittime' or 'admittime' is not null\n",
    "df_not_null = df[df['admittime'].notnull() & df['intime_ed'].notnull()]\n",
    "\n",
    "# Filter rows where the difference between 'admittime' and 'intime_ed' >= -1 hour\n",
    "df_result = df_not_null[(df_not_null['admittime'] - df_not_null['intime_ed']).dt.total_seconds() > (-3600*24)]\n",
    "\n",
    "# Combine the original DataFrame with df_not_null and the duplicates-filtered DataFrame\n",
    "df = pd.concat([df_null, df_result], ignore_index=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Features that are not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447444, 96)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop features deemeed unnecessary\n",
    "df = df.drop(['index', 'subject_id', 'hadm_id', 'stay_id', 'anchor_age', 'anchor_year', 'dod',\n",
    "              'admittime', 'dischtime', 'deathtime', 'race', 'edregtime', 'edouttime', 'in_year',\n",
    "              'before_ed_mortality', 'ed_los', 'intime_icu', 'outtime_icu', 'time_to_icu_transfer', 'time_to_icu_transfer_hours',\n",
    "              'next_ed_visit_time', 'next_ed_visit_time_diff', 'next_ed_visit_time_diff_days', 'chiefcomplaint'], axis=1)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mimic-extract\n",
    "vitals_valid_range = {\n",
    "    'temperature': {'outlier_low': 14.2, 'valid_low': 26, 'valid_high': 45, 'outlier_high':47},\n",
    "    'heartrate': {'outlier_low': 0, 'valid_low': 0, 'valid_high': 350, 'outlier_high':390},\n",
    "    'resprate': {'outlier_low': 0, 'valid_low': 0, 'valid_high': 300, 'outlier_high':330},\n",
    "    'o2sat': {'outlier_low': 0, 'valid_low': 0, 'valid_high': 100, 'outlier_high':150},\n",
    "    'sbp': {'outlier_low': 0, 'valid_low': 0, 'valid_high': 375, 'outlier_high':375},\n",
    "    'dbp': {'outlier_low': 0, 'valid_low': 0, 'valid_high': 375, 'outlier_high':375},\n",
    "    'pain': {'outlier_low': 0, 'valid_low': 0, 'valid_high': 10, 'outlier_high':10},\n",
    "    'acuity': {'outlier_low': 1, 'valid_low': 1, 'valid_high': 5, 'outlier_high':5},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_temp_to_celcius(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447444, 96)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = remove_outliers(df, vitals_valid_range)\n",
    "# what this really does is to transform values between valid low and outlier low to valid low,\n",
    "# and values between valid high and outlier high to valid high\n",
    "# anything beyond outlier low and outlier high will be set to NaN\n",
    "# however the code later imputes NaN with median. i have some concerns with this and would prefer to drop the rows with NaN\n",
    "# (it's approximately 12k rows only anyway)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Manipulation for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change True and False to Binary 1 and 0\n",
    "df = df.replace(True, 1)\n",
    "df = df.replace(False, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the replace() method to convert 'M' to 1 and 'F' to 0 in the 'gender' column\n",
    "df['gender'] = df['gender'].replace({'M': 1, 'F': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins and labels for age groups\n",
    "# Roughly based on NIH (US) age categories, however there is no concensus on ages above 65 years as stated on NIH website\n",
    "# Age Groups:\n",
    "# Young Adults: 18 to 34 years\n",
    "# Middle-Aged Adults: 35 to 64 years\n",
    "# Older Adults: 65 to 74 years\n",
    "# Elderly: 76 years and above\n",
    "bins = [18, 35, 65, 75, 100]  # The age group boundaries\n",
    "labels = ['Young Adults', 'Middle-Aged Adults', 'Older Adults', 'Elderly']\n",
    "\n",
    "# Use pd.cut() to bin the 'age' column and replace values with labels\n",
    "df['age_group'] = pd.cut(df['age'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447444, 102)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode Insurance Type\n",
    "df = pd.get_dummies(df, columns=['insurance'])\n",
    "\n",
    "# one-hot encode Age\n",
    "df = pd.get_dummies(df, columns=['age_group'])\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "triage_temperature     25110\n",
       "triage_heartrate       17963\n",
       "triage_resprate        21377\n",
       "triage_o2sat           21686\n",
       "triage_sbp             19242\n",
       "triage_dbp             20485\n",
       "triage_pain            41636\n",
       "triage_acuity           7313\n",
       "ed_temperature_last    28265\n",
       "ed_heartrate_last      19408\n",
       "ed_resprate_last       19914\n",
       "ed_o2sat_last          30657\n",
       "ed_sbp_last            19654\n",
       "ed_dbp_last            19930\n",
       "ed_pain_last           52277\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for Null Values\n",
    "df.isna().sum()[df.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is to convert intime_ed from object data type to datetime data type\n",
    "df['outtime_ed'] = pd.to_datetime(df['outtime_ed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code chunk is meant to check the data types of the columns in the data set and save them to a text file for reference.\n",
    "\n",
    "data_types = df.dtypes\n",
    "\n",
    "# Convert the data types to a DataFrame for better formatting\n",
    "data_types_df = pd.DataFrame(data_types, columns=['Data Type'])\n",
    "\n",
    "# Convert the DataFrame to a string\n",
    "data_types_str = data_types_df.to_string()\n",
    "\n",
    "# Save the string to a text file\n",
    "with open('data_typesv3.txt', 'w') as file:\n",
    "    file.write(data_types_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Option A: Split Train (80%) and Test (20%) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357955, 102)\n",
      "(89489, 102)\n"
     ]
    }
   ],
   "source": [
    "# Split Train and Test data\n",
    "df_train_impute=df.sample(frac=0.8,random_state=2023) #random state is a seed value\n",
    "df_test_impute=df.drop(df_train_impute.index)\n",
    "print(df_train_impute.shape)\n",
    "print(df_test_impute.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Option A: Impute NaNs with Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['triage_temperature',\n",
       " 'triage_heartrate',\n",
       " 'triage_resprate',\n",
       " 'triage_o2sat',\n",
       " 'triage_sbp',\n",
       " 'triage_dbp',\n",
       " 'triage_pain',\n",
       " 'ed_temperature_last',\n",
       " 'ed_heartrate_last',\n",
       " 'ed_resprate_last',\n",
       " 'ed_o2sat_last',\n",
       " 'ed_sbp_last',\n",
       " 'ed_dbp_last',\n",
       " 'ed_pain_last']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vitals_cols = [col for col in df.columns if len(col.split('_')) > 1 and \n",
    "                                                   col.split('_')[1] in vitals_valid_range and\n",
    "                                                   col.split('_')[1] != 'acuity']\n",
    "vitals_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "df_train_impute[vitals_cols] = imputer.fit_transform(df_train_impute[vitals_cols])\n",
    "df_test_impute[vitals_cols] = imputer.transform(df_test_impute[vitals_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Option A: Add Score values to Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable 'add_triage_MAP' successfully added\n",
      "Variable 'add_score_CCI' successfully added\n",
      "Variable 'Score_CART' successfully added\n",
      "Variable 'Score_REMS' successfully added\n",
      "Variable 'Score_NEWS' successfully added\n",
      "Variable 'Score_NEWS2' successfully added\n",
      "Variable 'Score_MEWS' successfully added\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(357955, 109)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_triage_MAP(df_train_impute) # add an extra variable MAP\n",
    "add_score_CCI(df_train_impute)\n",
    "add_score_CART(df_train_impute)\n",
    "add_score_REMS(df_train_impute)\n",
    "add_score_NEWS(df_train_impute)\n",
    "add_score_NEWS2(df_train_impute)\n",
    "add_score_MEWS(df_train_impute)\n",
    "\n",
    "df_train_impute.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Option A: Add Score values to Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable 'add_triage_MAP' successfully added\n",
      "Variable 'add_score_CCI' successfully added\n",
      "Variable 'Score_CART' successfully added\n",
      "Variable 'Score_REMS' successfully added\n",
      "Variable 'Score_NEWS' successfully added\n",
      "Variable 'Score_NEWS2' successfully added\n",
      "Variable 'Score_MEWS' successfully added\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(89489, 109)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_triage_MAP(df_test_impute) # add an extra variable MAP\n",
    "add_score_CCI(df_test_impute)\n",
    "add_score_CART(df_test_impute)\n",
    "add_score_REMS(df_test_impute)\n",
    "add_score_NEWS(df_test_impute)\n",
    "add_score_NEWS2(df_test_impute)\n",
    "add_score_MEWS(df_test_impute)\n",
    "\n",
    "df_test_impute.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Option A: Output the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_impute.to_csv('train_impute_v3.csv', index=False)\n",
    "df_test_impute.to_csv('test_impute_v3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Option B: Drop NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447444, 102)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341290, 102)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop all rows with nan values\n",
    "df_drop = df.dropna()\n",
    "df_drop.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Option B: Add Score values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuyi\\aha\\Project Notebooks\\helpers6.py:472: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['triage_MAP'] = df['triage_sbp']*1/3 + df['triage_dbp']*2/3\n",
      "C:\\Users\\zhuyi\\aha\\Project Notebooks\\helpers6.py:467: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['score_CCI'] = np.select(conditions, values)\n",
      "C:\\Users\\zhuyi\\aha\\Project Notebooks\\helpers6.py:468: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['score_CCI'] = df['score_CCI'] + df['cci_MI'] + df['cci_CHF'] + df['cci_PVD'] + df['cci_Stroke'] + df['cci_Dementia'] + df['cci_Pulmonary'] + df['cci_PUD'] + df['cci_Rheumatic'] +df['cci_Liver1']*1 + df['cci_Liver2']*3 + df['cci_DM1'] + df['cci_DM2']*2 +df['cci_Paralysis']*2 + df['cci_Renal']*2 + df['cci_Cancer1']*2 + df['cci_Cancer2']*6 + df['cci_HIV']*6\n",
      "C:\\Users\\zhuyi\\aha\\Project Notebooks\\helpers6.py:551: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['score_CART'] = np.select(conditions1, values1) + np.select(conditions2, values2) + np.select(conditions3, values3) +                              np.select(conditions4, values4)\n",
      "C:\\Users\\zhuyi\\aha\\Project Notebooks\\helpers6.py:520: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['score_REMS'] = np.select(conditions1, values1) + np.select(conditions2, values2) + np.select(conditions3, values3) +                              np.select(conditions4, values4) + np.select(conditions5, values5)\n",
      "C:\\Users\\zhuyi\\aha\\Project Notebooks\\helpers6.py:595: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['score_NEWS'] = np.select(conditions1, values1) + np.select(conditions2, values2) + np.select(conditions3, values3) +                              np.select(conditions4, values4) + np.select(conditions5, values5)\n",
      "C:\\Users\\zhuyi\\aha\\Project Notebooks\\helpers6.py:632: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['score_NEWS2'] = np.select(conditions1, values1) + np.select(conditions2, values2) + np.select(conditions3, values3) +                              np.select(conditions4, values4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable 'add_triage_MAP' successfully added\n",
      "Variable 'add_score_CCI' successfully added\n",
      "Variable 'Score_CART' successfully added\n",
      "Variable 'Score_REMS' successfully added\n",
      "Variable 'Score_NEWS' successfully added\n",
      "Variable 'Score_NEWS2' successfully added\n",
      "Variable 'Score_MEWS' successfully added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuyi\\aha\\Project Notebooks\\helpers6.py:667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['score_MEWS'] = np.select(conditions1, values1) + np.select(conditions2, values2) + np.select(conditions3, values3) +                              np.select(conditions4, values4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(341290, 109)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_triage_MAP(df_drop) # add an extra variable MAP\n",
    "add_score_CCI(df_drop)\n",
    "add_score_CART(df_drop)\n",
    "add_score_REMS(df_drop)\n",
    "add_score_NEWS(df_drop)\n",
    "add_score_NEWS2(df_drop)\n",
    "add_score_MEWS(df_drop)\n",
    "\n",
    "df_drop.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Option B: Split Train (80%) and Test (20%) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273032, 109)\n",
      "(68258, 109)\n"
     ]
    }
   ],
   "source": [
    "# Split Train and Test data\n",
    "df_train_drop=df_drop.sample(frac=0.8,random_state=2023) #random state is a seed value\n",
    "df_test_drop=df_drop.drop(df_train_drop.index)\n",
    "print(df_train_drop.shape)\n",
    "print(df_test_drop.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Option B: Output the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_drop.to_csv('train_drop_v3.csv', index=False)\n",
    "df_test_drop.to_csv('test_drop_v3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
